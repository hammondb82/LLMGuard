# llmguard

The way I processed the data was a bit convoluted so I wanted to run you through the procress. 

Procedure:

1. Base prompts from /{dataset_name}/prompts were fed into the Guarded-LLM
        -The "question" field served as the prompt for the base_set.
        -The "aug_q" field served as the prompt for the attacked_enhanced_set.
        -The "prompt" field was also used for the attacked_enhanced_set.
2. Responses were saved in /{dataset_name}/responses/{dataset_name}_responses.json
        
        a. Responses caught by the input or output guard had an "input" field added to their entry in the JSON file.
        
        b. Responses not caught by the output scanners did not have an "input" field.
3. Responses without the input field, indicating they were generated by the LLM, were filtered for grading:
        
        a. These were saved in /{dataset_name}/responses/{dataset_name}_past_guard_responses.json.
4. All past_guard_responses.json files were autograded using the following notebook: [SaladBench Testing](https://www.kaggle.com/code/bradhammond/saladbench-testing).
5. The graded responses were saved as a .csv file in /{dataset_name}/graded/.
6. Graded responses were then merged with the original responses into a dataframe. Responses not autograded were labeled as 0, indicating they were safe (code is in createFinalSpreadSheet.py).
7. The dataframes were cross-referenced with the original datasets to assign categories:
        
        a. This data was saved in /{dataset_name}/categorized/{dataset_name}_category_counts.csv.
8. Data from category_counts.csv were manually compiled into the final_categories.csv file in the directory: /{dataset_name}/categorized/.
9. Graphs were generated from the final_analysis.csv files.
